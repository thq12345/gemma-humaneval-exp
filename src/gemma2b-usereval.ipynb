{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-29T01:45:42.140819400Z",
     "start_time": "2024-03-29T01:45:37.876359300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\danny\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, load_metric\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfeff80e37f54fa3b0e9694e4b595fc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\envs\\transformer-final-project\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danny\\.cache\\huggingface\\hub\\models--google--gemma-7b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07d51712be0845869f35831c6992843d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b024c8dfcb046cda774db5ef56e370d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54bb02bbbc6e4e34a65bccd0ba06e98b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fde2fed91d1141538dab1bb68b9a61ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d363f6af2fe64b6ca666f920e5f2e513"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c42b121531914562802d6142ec10e0b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f419e5614c514fae91ea3392d56972b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdd0a9bf63264af2bc333bcc7094df11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aed8916e87d8436a8e4de0ca00349043"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1b10380e28042d89cec8208be835c3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f01ed22bc2345de94560296203ed5be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da04692ab5e949dc9573870895191561"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"cpu\")\n",
    "\n",
    "# Assuming CUDA is available, adjust if necessary\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T02:10:53.940444900Z",
     "start_time": "2024-03-29T01:45:43.890748300Z"
    }
   },
   "id": "9e6cff73189ad01e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       task_id                                             prompt  \\\n0  HumanEval/0  from typing import List\\n\\n\\ndef has_close_ele...   \n1  HumanEval/1  from typing import List\\n\\n\\ndef separate_pare...   \n2  HumanEval/2  \\n\\ndef truncate_number(number: float) -> floa...   \n3  HumanEval/3  from typing import List\\n\\n\\ndef below_zero(op...   \n4  HumanEval/4  from typing import List\\n\\n\\ndef mean_absolute...   \n\n                                  canonical_solution  \\\n0      for idx, elem in enumerate(numbers):\\n    ...   \n1      result = []\\n    current_string = []\\n    ...   \n2                              return number % 1.0\\n   \n3      balance = 0\\n\\n    for op in operations:\\n...   \n4      mean = sum(numbers) / len(numbers)\\n    re...   \n\n                                                test              entry_point  \n0  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...       has_close_elements  \n1  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...    separate_paren_groups  \n2  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...          truncate_number  \n3  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...               below_zero  \n4  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...  mean_absolute_deviation  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task_id</th>\n      <th>prompt</th>\n      <th>canonical_solution</th>\n      <th>test</th>\n      <th>entry_point</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HumanEval/0</td>\n      <td>from typing import List\\n\\n\\ndef has_close_ele...</td>\n      <td>for idx, elem in enumerate(numbers):\\n    ...</td>\n      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n      <td>has_close_elements</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HumanEval/1</td>\n      <td>from typing import List\\n\\n\\ndef separate_pare...</td>\n      <td>result = []\\n    current_string = []\\n    ...</td>\n      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n      <td>separate_paren_groups</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HumanEval/2</td>\n      <td>\\n\\ndef truncate_number(number: float) -&gt; floa...</td>\n      <td>return number % 1.0\\n</td>\n      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n      <td>truncate_number</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HumanEval/3</td>\n      <td>from typing import List\\n\\n\\ndef below_zero(op...</td>\n      <td>balance = 0\\n\\n    for op in operations:\\n...</td>\n      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n      <td>below_zero</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HumanEval/4</td>\n      <td>from typing import List\\n\\n\\ndef mean_absolute...</td>\n      <td>mean = sum(numbers) / len(numbers)\\n    re...</td>\n      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n      <td>mean_absolute_deviation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "humaneval_df = dataset['test'].to_pandas()\n",
    "humaneval_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T02:42:58.436011800Z",
     "start_time": "2024-03-29T02:42:54.492878700Z"
    }
   },
   "id": "20d31b636bbb190d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def remove_triple_quotes(text: str) -> str:\n",
    "    # Pattern to match triple double quotes and everything in between\n",
    "    pattern_double_quotes = r'\"\"\"(.*?)\"\"\"'\n",
    "    # Pattern to match triple single quotes and everything in between\n",
    "    pattern_single_quotes = r\"'''(.*?)'''\"\n",
    "    \n",
    "    # Remove triple double quotes and their contents\n",
    "    text_without_double_quotes = re.sub(pattern_double_quotes, '', text, flags=re.DOTALL)\n",
    "    # Remove triple single quotes and their contents\n",
    "    text_without_any_quotes = re.sub(pattern_single_quotes, '', text_without_double_quotes, flags=re.DOTALL)\n",
    "    \n",
    "    return text_without_any_quotes\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T02:43:04.936014200Z",
     "start_time": "2024-03-29T02:43:04.922015800Z"
    }
   },
   "id": "6a2ec05bba7a3060"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# def execute_code(generated_code, test, entry_point):\n",
    "#     local_vars = {}\n",
    "#     # Remove triple quotes if needed\n",
    "#     generated_code = remove_triple_quotes(generated_code)\n",
    "# \n",
    "#     exec(generated_code, globals(), local_vars)\n",
    "#     # print('generated_code is:', generated_code)\n",
    "#     # Replace 'candidate' with the actual function name in test code\n",
    "#     updated_test_code = test.replace('candidate', entry_point)\n",
    "#     # Execute the modified test code\n",
    "#     try:\n",
    "#         # print('test code is:', updated_test_code)\n",
    "#         exec(updated_test_code, globals(), local_vars)\n",
    "#         exec(f'check({entry_point})', globals(), local_vars)\n",
    "#         # If execution reaches this point, all asserts passed\n",
    "#         test_passed = True\n",
    "#     except AssertionError:\n",
    "#         # If any assert fails, an AssertionError is raised\n",
    "#         test_passed = False\n",
    "# \n",
    "#     # Return a simple boolean indicating if the test passed or failed\n",
    "#     return test_passed\n",
    "\n",
    "import threading\n",
    "\n",
    "def execute_code(generated_code, test, entry_point):\n",
    "    def target(local_vars):\n",
    "        try:\n",
    "            # define Gemma-generated code\n",
    "            exec(generated_code, globals(), local_vars)\n",
    "            \n",
    "            # define test\n",
    "            updated_test_code = test.replace('candidate', entry_point)\n",
    "            exec(updated_test_code, globals(), local_vars)\n",
    "            # run test\n",
    "            exec(f'check({entry_point})', globals(), local_vars)\n",
    "            \n",
    "            local_vars[\"test_passed\"] = True\n",
    "        except AssertionError:\n",
    "            local_vars[\"test_passed\"] = False\n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "            local_vars[\"test_passed\"] = False\n",
    "\n",
    "    local_vars = {}\n",
    "    # Remove triple quotes if needed\n",
    "    print('generated code is:', generated_code)\n",
    "    generated_code = remove_triple_quotes(generated_code)\n",
    "    test_thread = threading.Thread(target=target, args=(local_vars,))\n",
    "    test_thread.start()\n",
    "\n",
    "    # Wait for the specified timeout\n",
    "    test_thread.join(timeout=3)\n",
    "    if test_thread.is_alive():\n",
    "        print(\"Execution timed out\")\n",
    "        return False\n",
    "\n",
    "    # Return the result of the test execution\n",
    "    return local_vars.get(\"test_passed\", False)\n",
    "\n",
    "\n",
    "def predict_and_evaluate(task):\n",
    "    prompt = task[\"prompt\"]\n",
    "    test_code = task[\"test\"]\n",
    "    entry_point = task[\"entry_point\"]\n",
    "    \n",
    "    # Generate input_ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate code using the model\n",
    "    outputs = model.generate(input_ids, max_length=2000)\n",
    "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Execute generated code and compare outputs\n",
    "    try:\n",
    "        result = execute_code(generated_code, test_code, entry_point)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        result = False\n",
    "    \n",
    "    print('the result is', result)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T02:43:06.974789100Z",
     "start_time": "2024-03-29T02:43:06.952808700Z"
    }
   },
   "id": "ab2f20af49463c9b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "humaneval_df = humaneval_df.drop(index=[6])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T02:43:09.838936700Z",
     "start_time": "2024-03-29T02:43:09.823932600Z"
    }
   },
   "id": "226463cb9dad4da2"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\envs\\transformer-final-project\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 10\u001B[0m\n\u001B[0;32m      4\u001B[0m     task \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m: row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m: row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentry_point\u001B[39m\u001B[38;5;124m\"\u001B[39m: row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentry_point\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      8\u001B[0m     }\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecuting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend(\u001B[43mpredict_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Calculate and print the metric, e.g., accuracy\u001B[39;00m\n\u001B[0;32m     14\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(results) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(results)\n",
      "Cell \u001B[1;32mIn[5], line 71\u001B[0m, in \u001B[0;36mpredict_and_evaluate\u001B[1;34m(task)\u001B[0m\n\u001B[0;32m     68\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# Generate code using the model\u001B[39;00m\n\u001B[1;32m---> 71\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m generated_code \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# Execute generated code and compare outputs\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\transformer-final-project\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\transformer-final-project\\lib\\site-packages\\transformers\\generation\\utils.py:1527\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n\u001B[0;32m   1510\u001B[0m         input_ids,\n\u001B[0;32m   1511\u001B[0m         candidate_generator\u001B[38;5;241m=\u001B[39mcandidate_generator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1523\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1524\u001B[0m     )\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n\u001B[0;32m   1526\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[1;32m-> 1527\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_greedy_search(\n\u001B[0;32m   1528\u001B[0m         input_ids,\n\u001B[0;32m   1529\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mprepared_logits_processor,\n\u001B[0;32m   1530\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mprepared_stopping_criteria,\n\u001B[0;32m   1531\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[0;32m   1532\u001B[0m         eos_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[0;32m   1533\u001B[0m         output_scores\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_scores,\n\u001B[0;32m   1534\u001B[0m         output_logits\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_logits,\n\u001B[0;32m   1535\u001B[0m         return_dict_in_generate\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mreturn_dict_in_generate,\n\u001B[0;32m   1536\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m   1537\u001B[0m         streamer\u001B[38;5;241m=\u001B[39mstreamer,\n\u001B[0;32m   1538\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   1539\u001B[0m     )\n\u001B[0;32m   1541\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n\u001B[0;32m   1542\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\transformer-final-project\\lib\\site-packages\\transformers\\generation\\utils.py:2406\u001B[0m, in \u001B[0;36mGenerationMixin._greedy_search\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   2403\u001B[0m unfinished_sequences \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(batch_size, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   2404\u001B[0m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcache_position\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(cur_len, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 2406\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_has_unfinished_sequences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthis_peer_finished\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   2407\u001B[0m     \u001B[38;5;66;03m# prepare model inputs\u001B[39;00m\n\u001B[0;32m   2408\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[0;32m   2410\u001B[0m     \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\transformer-final-project\\lib\\site-packages\\transformers\\generation\\utils.py:1795\u001B[0m, in \u001B[0;36mGenerationMixin._has_unfinished_sequences\u001B[1;34m(self, this_peer_finished, synced_gpus, device)\u001B[0m\n\u001B[0;32m   1793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m this_peer_finished_flag\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[0;32m   1794\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1795\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m this_peer_finished:\n\u001B[0;32m   1796\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1797\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Assuming `df` is your DataFrame\n",
    "for index, row in humaneval_df.iloc[[105]].iterrows():\n",
    "    task = {\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"test\": row[\"test\"],\n",
    "        \"entry_point\": row[\"entry_point\"]\n",
    "    }\n",
    "    print(f\"executing {index}\")\n",
    "    results.append(predict_and_evaluate(task))\n",
    "\n",
    "\n",
    "# Calculate and print the metric, e.g., accuracy\n",
    "accuracy = sum(results) / len(results)\n",
    "print(f\"Model Accuracy on HumanEval: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T02:44:39.224221300Z",
     "start_time": "2024-03-29T02:43:14.511923800Z"
    }
   },
   "id": "2999336863debd0c"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T01:44:03.623396900Z",
     "start_time": "2024-03-29T01:44:03.598397400Z"
    }
   },
   "id": "ab391cff2a270b30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6856e567e354ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
